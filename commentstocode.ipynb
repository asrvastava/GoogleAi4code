{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"final.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMHEjIvIB1aTjwauR7Cg+Tn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"M5POFhVzSo1z"},"outputs":[],"source":["import json\n","from pathlib import Path\n","\n","import numpy as np\n","import pandas as pd\n","from scipy import sparse\n","from tqdm import tqdm\n","\n","pd.options.display.width = 180\n","pd.options.display.max_colwidth = 120\n","\n","BERT_PATH = \"../input/huggingface-bert-variants/distilbert-base-uncased/distilbert-base-uncased\"\n","\n","data_dir = Path('../input/AI4Code')"]},{"cell_type":"code","source":["NUM_TRAIN = 200\n","\n","\n","def read_notebook(path):\n","    return (\n","        pd.read_json(\n","            path,\n","            dtype={'cell_type': 'category', 'source': 'str'})\n","        .assign(id=path.stem)\n","        .rename_axis('cell_id')\n","    )\n","\n","\n","paths_train = list((data_dir / 'train').glob('*.json'))[:NUM_TRAIN]\n","notebooks_train = [\n","    read_notebook(path) for path in tqdm(paths_train, desc='Train NBs')\n","]\n","df = (\n","    pd.concat(notebooks_train)\n","    .set_index('id', append=True)\n","    .swaplevel()\n","    .sort_index(level='id', sort_remaining=False)\n",")\n","\n","df"],"metadata":{"id":"KJ3IUs_uSsr2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nb_id = df.index.unique('id')[6]\n","print('Notebook:', nb_id)\n","\n","print(\"The disordered notebook:\")\n","nb = df.loc[nb_id, :]\n","display(nb)\n","print()"],"metadata":{"id":"qapaVy6oS6Kj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(df_orders.loc[\"002ba502bdac45\"])"],"metadata":{"id":"MXWtUGDRTCmn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cell_order = df_orders.loc[nb_id]\n","\n","print(\"The ordered notebook:\")\n","nb.loc[cell_order, :]"],"metadata":{"id":"qXw-43SnTK8D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_ranks(base, derived):\n","    return [base.index(d) for d in derived]\n","​\n","cell_ranks = get_ranks(cell_order, list(nb.index))\n","nb.insert(0, 'rank', cell_ranks)\n","​\n","nb\n"],"metadata":{"id":"sxdOpKRXTMDc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_orders_ = df_orders.to_frame().join(\n","    df.reset_index('cell_id').groupby('id')['cell_id'].apply(list),\n","    how='right',\n",")\n","\n","ranks = {}\n","for id_, cell_order, cell_id in df_orders_.itertuples():\n","    ranks[id_] = {'cell_id': cell_id, 'rank': get_ranks(cell_order, cell_id)}\n","\n","df_ranks = (\n","    pd.DataFrame\n","    .from_dict(ranks, orient='index')\n","    .rename_axis('id')\n","    .apply(pd.Series.explode)\n","    .set_index('cell_id', append=True)\n",")\n","\n","df_ranks"],"metadata":{"id":"Wxo1-0ZiTSNE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_ancestors = pd.read_csv(data_dir / 'train_ancestors.csv', index_col='id')\n","df_ancestors\n"],"metadata":{"id":"69jcbd3-TXFA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = df.reset_index().merge(df_ranks, on=[\"id\", \"cell_id\"]).merge(df_ancestors, on=[\"id\"])\n","df"],"metadata":{"id":"THCNGFWGTiXt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df[\"pct_rank\"] = df[\"rank\"] / df.groupby(\"id\")[\"cell_id\"].transform(\"count\")\n","df[\"pct_rank\"].hist(bins=10)"],"metadata":{"id":"j8_E7R4kToOt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dict_cellid_source = dict(zip(df['cell_id'].values, df['source'].values))"],"metadata":{"id":"z6UrSzsxTxM_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import os\n","import re\n","# import fasttext\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","from sklearn.metrics.pairwise import cosine_similarity\n","from nltk.stem import WordNetLemmatizer\n","from pathlib import Path\n","import nltk\n","nltk.download('wordnet')\n","\n","stemmer = WordNetLemmatizer()\n","\n","def preprocess_text(document):\n","        # Remove all the special characters\n","        document = re.sub(r'\\W', ' ', str(document))\n","\n","        # remove all single characters\n","        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n","\n","        # Remove single characters from the start\n","        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n","\n","        # Substituting multiple spaces with single space\n","        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n","\n","        # Removing prefixed 'b'\n","        document = re.sub(r'^b\\s+', '', document)\n","\n","        # Converting to Lowercase\n","        document = document.lower()\n","        #return document\n","\n","        # Lemmatization\n","        tokens = document.split()\n","        tokens = [stemmer.lemmatize(word) for word in tokens]\n","        tokens = [word for word in tokens if len(word) > 3]\n","\n","        preprocessed_text = ' '.join(tokens)\n","        return preprocessed_text\n","\n","    \n","def preprocess_df(df):\n","    \"\"\"\n","    This function is for processing sorce of notebook\n","    returns preprocessed dataframe\n","    \"\"\"\n","    return [preprocess_text(message) for message in df.source]\n","\n","df.source = df.source.apply(preprocess_text)"],"metadata":{"id":"V23A_QxOTyMU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import GroupShuffleSplit\n","​\n","NVALID = 0.1  # size of validation set\n","​\n","splitter = GroupShuffleSplit(n_splits=1, test_size=NVALID, random_state=0)\n","​\n","train_ind, val_ind = next(splitter.split(df, groups=df[\"ancestor_id\"]))\n","​\n","train_df = df.loc[train_ind].reset_index(drop=True)\n","val_df = df.loc[val_ind].reset_index(drop=True)"],"metadata":{"id":"whXSQE0dT_8b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm.notebook import tqdm\n","\n","def generate_triplet(df, mode='train'):\n","  triplets = []\n","  ids = df.id.unique()\n","  random_drop = np.random.random(size=10000)>0.9\n","  count = 0\n","\n","  for id, df_tmp in tqdm(df.groupby('id')):\n","    df_tmp_markdown = df_tmp[df_tmp['cell_type']=='markdown']\n","\n","    df_tmp_code = df_tmp[df_tmp['cell_type']=='code']\n","    df_tmp_code_rank = df_tmp_code['rank'].values\n","    df_tmp_code_cell_id = df_tmp_code['cell_id'].values\n","\n","    for cell_id, rank in df_tmp_markdown[['cell_id', 'rank']].values:\n","      labels = np.array([(r==(rank+1)) for r in df_tmp_code_rank]).astype('int')\n","\n","      for cid, label in zip(df_tmp_code_cell_id, labels):\n","        count += 1\n","        if label==1:\n","          triplets.append( [cell_id, cid, label] )\n","          # triplets.append( [cid, cell_id, label] )\n","        elif mode == 'test':\n","          triplets.append( [cell_id, cid, label] )\n","          # triplets.append( [cid, cell_id, label] )\n","        elif random_drop[count%10000]:\n","          triplets.append( [cell_id, cid, label] )\n","          # triplets.append( [cid, cell_id, label] )\n","    \n","  return triplets\n","\n","triplets = generate_triplet(train_df)\n","val_triplets = generate_triplet(val_df, mode = 'test')"],"metadata":{"id":"mSj9R-z6UE_Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["val_df.head()\n"],"metadata":{"id":"RGT16hmxUP1K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from bisect import bisect\n","\n","\n","def count_inversions(a):\n","    inversions = 0\n","    sorted_so_far = []\n","    for i, u in enumerate(a):\n","        j = bisect(sorted_so_far, u)\n","        inversions += i - j\n","        sorted_so_far.insert(j, u)\n","    return inversions\n","\n","\n","def kendall_tau(ground_truth, predictions):\n","    total_inversions = 0\n","    total_2max = 0  # twice the maximum possible inversions across all instances\n","    for gt, pred in zip(ground_truth, predictions):\n","        ranks = [gt.index(x) for x in pred]  # rank predicted order in terms of ground truth\n","        total_inversions += count_inversions(ranks)\n","        n = len(gt)\n","        total_2max += n * (n - 1)\n","    return 1 - 4 * total_inversions / total_2max"],"metadata":{"id":"-sp-vw74UVjZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import RobertaTokenizer, RobertaConfig, RobertaModel\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import torch\n","from transformers import AutoModelWithLMHead, AutoTokenizer, AutoModel\n","\n","MAX_LEN = 128\n","\n","    \n","class MarkdownModel(nn.Module):\n","    def __init__(self):\n","        super(MarkdownModel, self).__init__()\n","        self.distill_bert = AutoModel.from_pretrained(\"../input/mymodelpairbertsmallpretrained/models/checkpoint-18000\")\n","        self.top = nn.Linear(512, 1)\n","\n","        self.dropout = nn.Dropout(0.2)\n","        \n","    def forward(self, ids, mask):\n","        x = self.distill_bert(ids, mask)[0]\n","        x = self.dropout(x)\n","        x = self.top(x[:, 0, :])\n","        x = torch.sigmoid(x) \n","        return x"],"metadata":{"id":"I4Bc4HU3Ud6v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader, Dataset\n","\n","\n","\n","class MarkdownDataset(Dataset):\n","    \n","    def __init__(self, df, max_len, mode='train'):\n","        super().__init__()\n","        self.df = df\n","        self.max_len = max_len\n","        self.tokenizer = AutoTokenizer.from_pretrained(\"../input/mymodelpairbertsmallpretrained/my_own_tokenizer\", do_lower_case=True)\n","        self.mode=mode\n","\n","    def __getitem__(self, index):\n","        row = self.df[index]\n","\n","        label = row[-1]\n","\n","        txt = dict_cellid_source[row[0]] + '[SEP]' + dict_cellid_source[row[1]]\n","\n","        inputs = self.tokenizer.encode_plus(\n","            txt,\n","            None,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            return_token_type_ids=True,\n","            truncation=True\n","        )\n","        ids = torch.LongTensor(inputs['input_ids'])\n","        mask = torch.LongTensor(inputs['attention_mask'])\n","\n","        return ids, mask, torch.FloatTensor([label])\n","\n","\n","\n","\n","    def __len__(self):\n","        return len(self.df)\n"],"metadata":{"id":"DmZWzUm3UuPh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def adjust_lr(optimizer, epoch):\n","    if epoch < 1:\n","        lr = 5e-5\n","    elif epoch < 2:\n","        lr = 1e-3\n","    elif epoch < 5:\n","        lr = 1e-4\n","    else:\n","        lr = 1e-5\n","\n","    for p in optimizer.param_groups:\n","        p['lr'] = lr\n","    return lr\n","    \n","def get_optimizer(net):\n","    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=3e-4, betas=(0.9, 0.999),\n","                                 eps=1e-08)\n","    return optimizer\n","\n","BS = 128\n","NW = 8"],"metadata":{"id":"Suag_1_NU_OE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def read_data(data):\n","    return tuple(d.cuda() for d in data[:-1]), data[-1].cuda()\n","\n","\n","def validate(model, val_loader, mode='train'):\n","    model.eval()\n","    \n","    tbar = tqdm(val_loader, file=sys.stdout)\n","    \n","    preds = np.zeros(len(val_loader.dataset), dtype='float32')\n","    labels = []\n","    count = 0\n","\n","    with torch.no_grad():\n","        for idx, data in enumerate(tbar):\n","            inputs, target = read_data(data)\n","\n","            pred = model(inputs[0], inputs[1]).detach().cpu().numpy().ravel()\n","\n","            preds[count:count+len(pred)] = pred\n","            count += len(pred)\n","            \n","            if mode=='test':\n","              labels.append(target.detach().cpu().numpy().ravel())\n","    if mode=='test':\n","      return preds\n","    else:\n","      return np.concatenate(labels), np.concatenate(preds)"],"metadata":{"id":"VWvd2GreVIYB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["paths_test = list((data_dir / 'test').glob('*.json'))\n","notebooks_test = [\n","    read_notebook(path) for path in tqdm(paths_test, desc='Test NBs')\n","]\n","test_df = (\n","    pd.concat(notebooks_test)\n","    .set_index('id', append=True)\n","    .swaplevel()\n","    .sort_index(level='id', sort_remaining=False)\n",").reset_index()"],"metadata":{"id":"mD2fPXfRVNGs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_df.source = test_df.source.apply(preprocess_text)\n","dict_cellid_source = dict(zip(test_df['cell_id'].values, test_df['source'].values))"],"metadata":{"id":"PArN22YOVOY4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_df[\"rank\"] = test_df.groupby([\"id\", \"cell_type\"]).cumcount()\n","test_df[\"pred\"] = test_df.groupby([\"id\", \"cell_type\"])[\"rank\"].rank(pct=False)"],"metadata":{"id":"pgnpR-yvVUIT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_triplets = generate_triplet(test_df, mode = 'test')"],"metadata":{"id":"B-im9mhYVXmX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_df[\"pct_rank\"] = 0\n","test_ds = MarkdownDataset(test_triplets, max_len=MAX_LEN)\n","test_loader = DataLoader(test_ds, batch_size=BS * 4, shuffle=False, num_workers=NW,\n","                          pin_memory=False, drop_last=False)\n","\n","\n","import gc \n","gc.collect()\n","len(test_ds), test_ds[0]"],"metadata":{"id":"HIcQ3y5NVdAW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sys \n","\n","model = MarkdownModel()\n","model = model.cuda()\n","model.load_state_dict(torch.load('../input/mymodelbertsmallpretrained120000/my_own_model.bin'))\n","y_test = validate(model, test_loader, mode='test')\n"],"metadata":{"id":"GCpQC_qYVoTz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["preds_copy = y_test"],"metadata":{"id":"4xS1DtsdVwTR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred_vals = []\n","count = 0\n","for id, df_tmp in tqdm(test_df.groupby('id')):\n","  df_tmp_mark = df_tmp[df_tmp['cell_type']=='markdown']\n","  df_tmp_code = df_tmp[df_tmp['cell_type']!='markdown']\n","  df_tmp_code_rank = df_tmp_code['rank'].rank().values\n","  N_code = len(df_tmp_code_rank)\n","  N_mark = len(df_tmp_mark)\n","\n","  preds_tmp = preds_copy[count:count+N_mark * N_code]\n","\n","  count += N_mark * N_code\n","\n","  for i in range(N_mark):\n","    pred = preds_tmp[i*N_code:i*N_code+N_code] \n","\n","    softmax = np.exp((pred-np.mean(pred)) *20)/np.sum(np.exp((pred-np.mean(pred)) *20)) \n","\n","    rank = np.sum(softmax * df_tmp_code_rank)\n","    pred_vals.append(rank)\n","\n","del model\n","del test_triplets[:]\n","del dict_cellid_source\n","gc.collect()"],"metadata":{"id":"tuEzbSx5Vxt2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_df.loc[test_df[\"cell_type\"] == \"markdown\", \"pred\"] = pred_vals"],"metadata":{"id":"D_db40rOV6MV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sub_df = test_df.sort_values(\"pred\").groupby(\"id\")[\"cell_id\"].apply(lambda x: \" \".join(x)).reset_index()\n","sub_df.rename(columns={\"cell_id\": \"cell_order\"}, inplace=True)\n","sub_df.head()"],"metadata":{"id":"T1AhV8HBWAA1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sub_df.to_csv(\"submission.csv\", index=False)"],"metadata":{"id":"VbghGnG9WGCT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"lis31UI-WHiu"},"execution_count":null,"outputs":[]}]}